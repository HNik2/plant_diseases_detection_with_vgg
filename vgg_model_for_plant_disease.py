# -*- coding: utf-8 -*-
"""vgg_model_for_plant_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E1yPKEDr2YHSGayA1lruTTiH7-oI8aUB
"""

# Import des bibliothèques nécessaire
import numpy as np 
import pandas as pd 
import seaborn as sns
import sys
import os
from numpy import load
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from keras import backend
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.callbacks import ModelCheckpoint
from keras.layers import Dropout
from keras.layers.normalization import BatchNormalization

from google.colab import drive
drive.mount('/content/drive')

# Copie du jeu de données des maladies des plantes dans google colaboratory
cp /content/drive/My\ Drive/dataset.zip /

# Copie du jeu de données de test des maladies des plantes dans google colaboratory
cp /content/drive/My\ Drive/test.zip /

import IPython.display as display
from PIL import Image
import os
import zipfile
import pathlib

#Décompression du jeu de données
local_zip = '/dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/')
zip_ref.close()

#Décompression du jeu de données pour les test
local_zip = '/test.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/')
zip_ref.close()

#Définition des répertoires pour les données d'entrainement et les données de validation et pour les test
train_dir = '/dataset/train/'
valid_dir = '/dataset/valid/'
test_dir = '/test'

# definition du modèle cnn à partir du modèle vgg
def define_model(in_shape=(224, 224, 3), out_shape=38):
	# chargement du modèle vgg
  model = VGG16(weights='imagenet',input_shape=in_shape, include_top=False)
  for layer in model.layers:
    layer.trainable = False
	
	# permettre au dernier bloc vgg d'être entraînable
  model.get_layer('block5_conv1').trainable=True
  model.get_layer('block5_conv2').trainable=True
  model.get_layer('block5_conv3').trainable=True
  model.get_layer('block5_pool').trainable=True

	#Ajout des nouvelles couches de classification
  flat1=Flatten()(model.layers[-1].output)
  fcon1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)
  fdrop1 = Dropout(0.25)(fcon1)
  fbn1 = BatchNormalization()(fdrop1)
  fcon2 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(fbn1)
  fdrop2 = Dropout(0.25)(fcon2)
  fbn2 = BatchNormalization()(fdrop2)
  output = Dense(out_shape, activation='softmax')(fbn2)
  #définition de notre modèle
  model = Model(inputs=model.inputs, outputs=output)
	#compiler le modèle
  opt = SGD(lr=0.01, momentum=0.9,decay=0.005)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

# fonction pour afficher les graphes de la précision et l'erreur de l'entrainement
def summarize_diagnostics(history):
  sns.set()
  # plot le graphe pour l'erreur
  pyplot.subplot(211)
  pyplot.title('Cross Entropy Loss')
  pyplot.plot(history.history['loss'], color='blue', label='train')
  pyplot.plot(history.history['val_loss'], color='orange', label='valid')
  pyplot.xlabel('Epoch')
  pyplot.ylabel('Loss')
  pyplot.legend()
  # plot le graphe pour la précsion
  pyplot.subplot(212)
  pyplot.title('Classification Accuracy')
  pyplot.plot(history.history['acc'], color='blue', label='train')
  pyplot.plot(history.history['val_acc'], color='orange', label='valid')
  pyplot.xlabel('Epoch')
  pyplot.ylabel('Accuracy')
  pyplot.legend()
  # sauvegarder le résultat dans un fichier png
  filename = sys.argv[0].split('/')[-1]
  pyplot.savefig(filename + '_plot.png')
  pyplot.close()

# charger les données pour tester la prédiction de notre modèle
def load_image(filename):
	# chargement de l'image
	img = load_img(filename, target_size=(224, 224))
	# convert to array
	img = img_to_array(img)
	# redimensionner la taille de l'image
	img = img.reshape(1, 224, 224, 3)
    # normalisation
	img = img/255
	return img

#prétraitement des images du jeu de données avant de le passer en entrée de notre modèle
batch_size = 128

train_datagen = ImageDataGenerator(rescale=1./255)
                                   
valid_datagen = ImageDataGenerator(rescale=1./255)

training_iterator = train_datagen.flow_from_directory(train_dir,
                                                 target_size=(224, 224),
                                                 batch_size=batch_size,
                                                 class_mode='categorical')

test_iterator = valid_datagen.flow_from_directory(valid_dir,
                                            target_size=(224, 224),
                                            batch_size=batch_size,
                                            class_mode='categorical')

#Affichage des différentes classes du jeu de données
class_dict = training_iterator.class_indices
print(class_dict)

#Affichage des différentes classes du jeu de données
class_labels = list(class_dict.keys())
print(class_labels)

train_num_samples = training_iterator.samples
valid_num_samples = test_iterator.samples
# création du modèle
model = define_model()
model.summary()

weightsfilepath = "/bestweights.hdf5"
checkpoint = ModelCheckpoint(weightsfilepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')
callbacks_list = [checkpoint]

#Entrainement du modèle
history = model.fit_generator(training_iterator, steps_per_epoch=len(training_iterator),
		validation_data=test_iterator, validation_steps=len(test_iterator), epochs=8, callbacks=callbacks_list, verbose=2)

# évaluation du modèle
_, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=1)
print('> %.3f' % (acc * 100.0))

# graphes de précision et d'erreur du modèle
summarize_diagnostics(history)

#sauvegarde du modèle
model.save('plantdisease_detection_vgg16model.h5')

# Test de prédiction
img = load_image('../test/AppleScab3.JPG')
print("Prediction for AppleScab3:")
prediction = model.predict(img)
predicted_class_name = class_labels[np.argmax(prediction)]
print("Detected the leaf as ", predicted_class_name)

for filename in os.listdir(test_dir):
    filepath = test_dir + '/' + filename
    img = load_image(filepath)
    prediction = model.predict(img)
    predicted_class_name = class_labels[np.argmax(prediction)]
    print(filename, "  predicted as ", predicted_class_name)

#Téléchargement du modèle
from google.colab import files
files.download('plantdisease_detection_vgg16model.h5')

#Convertion du modèle en format tflite
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_keras_model_file('plantdisease_detection_vgg16model.h5')
tflite_model = converter.convert()

with open('model_vgg.tflite', 'wb') as f:
  f.write(tflite_model)

#Création du fichier labels contenant les différentes classes de maladies
print (training_iterator.class_indices)

labels = '\n'.join(sorted(training_iterator.class_indices.keys()))

with open('labels.txt', 'w') as f:
  f.write(labels)

#Téléchargement du modèle tflite et du fichier labels
from google.colab import files

files.download('model_vgg.tflite')
files.download('labels.txt')